May 2, 2021
I have decided to rewrite the neural network module for organization purposes. I am also going to properly implament CNN functions into this version as well as add compatibility with multiple gradient and layer functions. I will be also adding a new model object as well as different layer type objects. currently I have gotten image pslit and such working in the new version and have began thinking up a new way of organizing all of the new function types. I'm thinking of making a model object which you then add layer objects to which would allow me to give each layer object it's own layer execution method as well as a gradient method to calculate their gradient, this should also allow me to store esential variables such as weights, the activation function, and I'm still thinking about it but I might also add the ability to store custom args such as velocity and alowing for more clean and organized code in the regard.

May 4, 2021
I have succcessfully transfered the fully connected layer into its own object in NeuralNetwork.py, I'm now going to try to learn more about dot products and derrivatives with hopes of solving the sum mystery. For context the "sum mystery" is basically just me confused out of my mind by why there's a sum function in my code that makes it work, if the sum function isn't there the shape is all wrong and the broadcast will obviously fail, but the reason why the sum is there on a more derivation level is still unknown. Luckily today I got a lead on this mystery when I found that it maybe linked to this theorem describing the derrivative of vector functions that utilize the dot product operation. I hope to solve this "mystery" by tomorrow.That's all for today I'm a bit busy so work is slow, I'll need to pick up the pace, (I say as I cram for my english test tomorrow).
